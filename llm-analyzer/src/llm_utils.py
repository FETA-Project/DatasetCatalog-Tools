"""
Module containing functions which handle communication with LLM.
"""
import re
import json
import time
import requests
import toml
from requests import ReadTimeout
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
from general_utils import substitute_patterns
from logging_utils import setup_logger
import constants as c
from constants import LLM_REQUEST_MAX_RETRIES

logger = setup_logger(__name__, f"{c.LOG_DIRECTORY}{__name__}{c.LOG_FILE_SUFFIX}", level=c.LOG_LEVEL)

def post_with_retries(url: str, payload_json: str, headers: dict, timeout: float, max_retries: int):
    """
    Sends HTTP POST request. If timeout is reached without response, function will retry until max_retries is reached.

    :param url:
        URL, to which the request will be sent
    :type url:
        str
    :param payload_json:
        JSON payload which will be sent in the request
    :type payload_json:
        str
    :param headers:
        HTTP headers which will be sent in request
    :type headers:
        dict
    :param timeout:
        Request timeout in seconds
    :type timeout:
        float
    :param max_retries:
        Maximum number of retries to perform.
    :type max_retries:
        int
    :return:
        HTTP response or None if max_retries is reached without success
    :rtype:
        str in case of success, None in case of failure
    """
    # define the retry strategy
    retry_strategy = Retry(
        total= 1,  # maximum number of retries
        backoff_factor=2,
        status_forcelist=[
            429,
            500,
            502,
            503,
            504,
        ],  # the HTTP status codes to retry on
    )

    # create a HTTP adapter with the retry strategy and mount it to the session
    adapter = HTTPAdapter(max_retries=retry_strategy)

    # create a new session object
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    # make a request using the session object
    for i in range(max_retries):
        try:
            response = session.post(url, data=payload_json, headers=headers, timeout=timeout)
            return response
        except ReadTimeout:
            print(f"request timed out, attempt {i}")
    return None


def llm_query(text: str, sys_prompt: str, prompt: str, field_name: str = "", response_type: str = "string"):
    """
    Sends given query to Ollama server.
    JSON response format is added for response quality enhancement.

    :param text:
        Context which will sent to LLM as part of the prompt
    :type text:
        str
    :param sys_prompt:
        System prompt
    :type sys_prompt:
        str
    :param prompt:
        Prompt for LLM
    :type prompt:
        str
    :param field_name:
        Name of the analysis field which the prompt is for
    :type field_name:
        str
    :param response_type:
        Data type of the wanted response (string, integer, ...)
    :type str:
        str
    :return:
        Extracted response of the LLM
    :rtype:
        str
    """
    #return "<think></think>dummy response"
    system_prompt = sys_prompt
    logger.info("system prompt is: %s\n", system_prompt)
    logger.info("prompt is: %s\n", prompt)
    logger.info("context is: %s\n", text)
    prompt = prompt + """\n
    Respond in a JSON format as provided.\n
    ### Text:
    \n""" + text
    logger.info("full prompt is: %s\n", prompt)
    config = toml.load(c.LLM_CONFIG_FILE_NAME)
    url = config[c.URL_OPTION_NAME]

    response_format = {
        "type": "object",
        "properties": {
            field_name: {
                "type": response_type
            }
        },
        "required": [
            field_name
        ]
    }

    payload = {
        "model": config[c.MODEL_NAME_OPTION_NAME],
        "prompt": prompt,
        "system": system_prompt,
        "stream": False,
        "options": config[c.MODEL_OPTIONS_OPTION_NAME],
        "format": response_format
    }
    payload_json = json.dumps(payload)
    headers = {"Content-Type": "application/json"}
    timeout = int(config[c.LLM_REQUEST_TIMEOUT_OPTION_NAME])
    max_retries = int(config[LLM_REQUEST_MAX_RETRIES])
    print("generating response...")
    start = time.time()
    response = post_with_retries(url, payload_json, headers, timeout, max_retries)
    end = time.time()
    elapsed = end - start
    if response is None:
        return "<think></think>request timed out"
    print(f"response generated in {elapsed//60} min, {elapsed%60:.2f} s.")
    logger.info(f"response generated in {elapsed//60} min, {elapsed%60:.2f} s.")
    logger.info("full response is: %s\n", json.loads(response.text)["response"])
    return str(json.loads(response.text)["response"])


def extract_answer(output: str, json_field: str = ""):
    """
    Extracts answer from JSON response generated by LLM.

    :param output:
        LLM generated JSON response
    :type output:
        str
    :param json_field:
        Name of the analysis field of which value will be extracted
    :type json_field:
        str
    :return:
        Response extracted from JSON template.
    :rtype:
        str
    """
    config_file_name = c.LLM_CONFIG_FILE_NAME
    config = toml.load(config_file_name)
    if json_field != "":
        try:
            # extract response from json template
            json_dict = json.loads(output)
            output = json_dict[json_field]
        except json.decoder.JSONDecodeError as decodeError:
            print("Invalid JSON output received from LLM, passing whole template to output analysis")
            # erasing all " that could corrupt the output
            output = output.replace('\\"', '')
            output = output.replace('"', '')
            pass
    # erasing thinking section from reasoning models
    delimiter = config[c.THINK_SECTION_DELIMITER_OPTION_NAME]
    output = substitute_patterns(config_file_name, output)
    if delimiter == "":
        return output
    else:
        return output.split(delimiter)[1].strip()